{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":"<p>Welcome \u2014 this site contains cheatsheets for convex optimisation.</p> <ul> <li>Cheatsheet_norm</li> <li>Duality_concept</li> <li>chapter4_boyd</li> <li>convexity_preserving_operations</li> <li>Strong_convexity</li> <li>Convex_sets_and_functions</li> </ul>"},{"location":"Convex_sets_and_functions/","title":"Convex_sets_and_functions","text":""},{"location":"Convex_sets_and_functions/#0-quick-symbols-i-keep-using","title":"0) Quick symbols I keep using","text":"<ul> <li>Convex combination:  \\(\\theta x + (1-\\theta)y\\), \\(\\theta\\in[0,1]\\)</li> <li>Affine combination: \\(\\theta x + (1-\\theta)y\\), \\(\\theta\\in\\mathbb{R}\\)</li> <li>Epigraph:  \\(\\mathrm{epi}\\,f=\\{(x,t)\\mid f(x)\\le t\\}\\)</li> <li>Hypograph: \\(\\mathrm{hypo}\\,f=\\{(x,t)\\mid f(x)\\ge t\\}\\)</li> <li>Sublevel set: \\(S_\\alpha=\\{x\\mid f(x)\\le \\alpha\\}\\)</li> <li>Superlevel set: \\(U_\\alpha=\\{x\\mid f(x)\\ge \\alpha\\}\\)</li> </ul>"},{"location":"Convex_sets_and_functions/#part-a-convex-sets-what-you-prove-100-times","title":"PART A \u2014 Convex sets (what you prove 100 times)","text":""},{"location":"Convex_sets_and_functions/#1-k-point-convexity-from-the-2-point-definition-exercise-21-idea","title":"1) \u201ck-point convexity\u201d from the 2-point definition (exercise 2.1 idea)","text":"<p>Given: \\(C\\subseteq\\mathbb{R}^n\\) convex in the usual sense (2-point). Show: for \\(x_1,\\dots,x_k\\in C\\), \\(\\theta_i\\ge0\\), \\(\\sum_{i=1}^k\\theta_i=1\\), [ \\sum_{i=1}^k\\theta_i x_i\\in C. ]</p> <p>Induction template - Base \\(k=2\\): exactly convexity definition. - Step \\(k\\to k+1\\): let \\(\\bar\\theta=\\sum_{i=1}^k\\theta_i = 1-\\theta_{k+1}\\).   - If \\(\\bar\\theta=0\\): then \\(\\theta_{k+1}=1\\) and the point is \\(x_{k+1}\\in C\\).   - Else define normalized weights \\(\\tilde\\theta_i=\\theta_i/\\bar\\theta\\) for \\(i\\le k\\),     so \\(\\sum_{i=1}^k\\tilde\\theta_i=1\\).   - By induction, \\(z=\\sum_{i=1}^k\\tilde\\theta_i x_i \\in C\\).   - Then     [     \\sum_{i=1}^{k+1}\\theta_i x_i     = \\bar\\theta\\, z + \\theta_{k+1}x_{k+1},     ]     which is a 2-point convex combination of \\(z\\in C\\) and \\(x_{k+1}\\in C\\),     hence is in \\(C\\).</p> <p>This proof is the one I reuse whenever I need \u201cfinite convex combinations\u201d.</p>"},{"location":"Convex_sets_and_functions/#2-voronoi-halfspace-exercise-27","title":"2) Voronoi halfspace (exercise 2.7)","text":"<p>Set: [ {x\\mid |x-a|_2\\le |x-b|_2}. ]</p> <p>Square both sides: [ |x-a|_2^2 \\le |x-b|_2^2 ] [ (x-a)^T(x-a)\\le (x-b)^T(x-b). ]</p> <p>Expand and cancel \\(x^Tx\\): [ -2a^Tx + a^Ta \\le -2b^Tx + b^Tb ] [ 2(b-a)^T x \\le b^Tb - a^Ta. ]</p> <p>So it\u2019s the (closed) halfspace [ (b-a)^T x \\le \\frac{|b|_2^2-|a|_2^2}{2}. ]</p> <p>Geometric: boundary is the perpendicular bisector hyperplane.</p>"},{"location":"Convex_sets_and_functions/#3-polyhedra-check-style-exercise-28-vibe","title":"3) Polyhedra check style (exercise 2.8 vibe)","text":"<p>A set is a polyhedron if it can be written as [ S={x\\mid Ax\\le b,\\ Fx=g}. ]</p>"},{"location":"Convex_sets_and_functions/#a-sy_1a_1y_2a_2mid-1le-y_1le-1-1le-y_2le-1","title":"(a) \\(S=\\{y_1a_1+y_2a_2\\mid -1\\le y_1\\le 1,\\ -1\\le y_2\\le 1\\}\\)","text":"<p>This is the linear image of a box: [ y\\in[-1,1]^2,\\quad x=Ay,\\ A=[a_1\\ a_2]. ] Since \\([-1,1]^2\\) is a polyhedron and linear maps preserve polyhedrality, \\(S\\) is a polyhedron.</p> <p>A direct explicit form is possible but not necessary; the \u201clinear image of a polyhedron\u201d argument is already enough.</p>"},{"location":"Convex_sets_and_functions/#b-xge0-mathbf1tx1-sum-x_i-a_ib_1-sum-x_i-a_i2b_2","title":"(b) \\(x\\ge0,\\ \\mathbf{1}^Tx=1,\\ \\sum x_i a_i=b_1,\\ \\sum x_i a_i^2=b_2\\)","text":"<p>All are linear equalities/inequalities in \\(x\\). So it is a polyhedron: [ \\underbrace{x\\ge0}{Ax\\le b},\\qquad \\underbrace{\\begin{bmatrix}\\mathbf{1}^T\\ a^T\\ (a^{\\circ 2})^T\\end{bmatrix}x=\\begin{bmatrix}1\\ b_1\\ b_2\\end{bmatrix}}{Fx=g}. ]</p>"},{"location":"Convex_sets_and_functions/#c-xge0-xtyle-1-forall-yy_21","title":"(c) \\(x\\ge0,\\ x^Ty\\le 1\\ \\forall y:\\|y\\|_2=1\\)","text":"<p>Use dual norm: [ \\sup_{|y|_2=1} x^Ty = |x|_2. ] So condition \u201cfor all unit \\(y\\)\u201d is equivalent to \\(\\|x\\|_2\\le 1\\) plus \\(x\\ge0\\). That set is convex but not polyhedral (Euclidean ball boundary is curved).</p>"},{"location":"Convex_sets_and_functions/#d-xge0-xtyle-1-forall-ysum-y_i1","title":"(d) \\(x\\ge0,\\ x^Ty\\le 1\\ \\forall y:\\sum |y_i|=1\\)","text":"<p>Here the constraint is [ \\sup_{|y|1=1} x^Ty = |x|\\infty \\le 1. ] So it becomes [ x\\ge0,\\quad x_i\\le 1\\ \\forall i, ] which is polyhedral.</p>"},{"location":"Convex_sets_and_functions/#4-classic-product-1-convex-set-exercise-211","title":"4) Classic \u201cproduct \u2265 1\u201d convex set (exercise 2.11)","text":"<p>Set \\(S=\\{(x_1,x_2)\\in\\mathbb{R}^2_{++}\\mid x_1x_2\\ge 1\\}\\).</p> <p>Take two points \\(u=(u_1,u_2)\\), \\(v=(v_1,v_2)\\) in \\(S\\). We need \\((\\theta u + (1-\\theta)v)\\in S\\), i.e. [ (\\theta u_1 + (1-\\theta)v_1)(\\theta u_2 + (1-\\theta)v_2)\\ge 1. ]</p> <p>Useful path (the hint is AM\u2013GM): - Since \\(u_1u_2\\ge1\\) and \\(v_1v_2\\ge1\\), then   [   (u_1u_2)^\\theta (v_1v_2)^{1-\\theta}\\ge 1.   ] - Rewrite:   [   (u_1^\\theta v_1^{1-\\theta})(u_2^\\theta v_2^{1-\\theta})\\ge 1.   ] - Apply weighted AM\u2013GM on each coordinate:   [   u_1^\\theta v_1^{1-\\theta}\\le \\theta u_1+(1-\\theta)v_1,   ]   [   u_2^\\theta v_2^{1-\\theta}\\le \\theta u_2+(1-\\theta)v_2.   ] - Multiply inequalities:   [   (u_1^\\theta v_1^{1-\\theta})(u_2^\\theta v_2^{1-\\theta})   \\le (\\theta u_1+(1-\\theta)v_1)(\\theta u_2+(1-\\theta)v_2).   ] Combine with the earlier \\(\\ge 1\\) to get the desired product \\(\\ge1\\). So \\(S\\) is convex.</p>"},{"location":"Convex_sets_and_functions/#part-b-convex-functions-how-you-recognize-convexity","title":"PART B \u2014 Convex functions (how you recognize convexity)","text":""},{"location":"Convex_sets_and_functions/#5-epigraph-facts-you-keep-using","title":"5) Epigraph facts you keep using","text":"<ul> <li>\\(f\\) is convex  \\(\\iff\\) \\(\\mathrm{epi}(f)\\) is convex.</li> <li>\\(f\\) is concave \\(\\iff\\) \\(\\mathrm{hypo}(f)\\) is convex.</li> <li>\\(f\\) is quasiconvex \\(\\iff\\) all sublevel sets \\(S_\\alpha\\) are convex.</li> <li>\\(f\\) is quasiconcave \\(\\iff\\) all superlevel sets \\(U_\\alpha\\) are convex.</li> </ul> <p>Halfspace / cone / polyhedron epigraph test (exercise 3.6 idea) - \\(\\mathrm{epi}(f)\\) is a halfspace \\(\\iff f\\) is affine (since a halfspace boundary is a hyperplane \\(t=a^Tx+b\\)). - \\(\\mathrm{epi}(f)\\) is a polyhedron \\(\\iff f\\) is polyhedral convex, i.e. \\(f(x)=\\max_i(a_i^Tx+b_i)\\). - \\(\\mathrm{epi}(f)\\) is a convex cone \\(\\iff f\\) is convex + positively homogeneous: \\(f(\\lambda x)=\\lambda f(x)\\) for \\(\\lambda\\ge0\\),   and \\(f(0)=0\\). (Then epi is closed under conic scaling.)</p>"},{"location":"Convex_sets_and_functions/#6-linear-fractional-example-why-open-and-closed-halfspaces-show-up","title":"6) Linear-fractional example (why \u201copen\u201d and \u201cclosed\u201d halfspaces show up)","text":"<p>For [ f(x)=\\frac{a^Tx+b}{c^Tx+d},\\qquad \\mathrm{dom}f={x\\mid c^Tx+d&gt;0}. ]</p> <p>\\(\\alpha\\)-sublevel set: [ S_\\alpha=\\left{x\\mid c^Tx+d&gt;0,\\ \\frac{a^Tx+b}{c^Tx+d}\\le \\alpha\\right}. ]</p> <p>Because \\(c^Tx+d&gt;0\\), multiply without flipping sign: [ a^Tx+b \\le \\alpha(c^Tx+d) \\iff (a-\\alpha c)^Tx \\le \\alpha d - b. ]</p> <p>So [ S_\\alpha = \\underbrace{{x\\mid c^Tx+d&gt;0}}{\\text{open halfspace}} \\ \\cap\\  \\underbrace{{x\\mid (a-\\alpha c)^Tx \\le \\alpha d-b}}{\\text{closed halfspace}}. ]</p> <p>Intersection is convex \\(\\Rightarrow\\) \\(f\\) is quasiconvex. (Same logic for superlevel sets gives quasiconcave too \u2192 quasilinear.)</p>"},{"location":"Convex_sets_and_functions/#7-check-on-lines-principle-quasiconvex","title":"7) \u201cCheck on lines\u201d principle (quasiconvex)","text":"<p>\\(f\\) is quasiconvex iff its restriction to any line is quasiconvex.</p> <p>Reason: sublevel sets convex in \\(\\mathbb{R}^n\\) iff their intersection with any line is convex in \\(\\mathbb{R}\\). So to test quasiconvexity you can reduce to 1D along \\(x(t)=x_0+tv\\).</p>"},{"location":"Convex_sets_and_functions/#8-differentiable-quasiconvex-condition-why-its-not-automatic","title":"8) Differentiable quasiconvex condition (why it\u2019s NOT automatic)","text":"<p>Claim (Boyd 3.4.3): [ f(y)\\le f(x)\\ \\Rightarrow\\ \\nabla f(x)^T(y-x)\\le 0. ]</p> <p>Why it\u2019s meaningful: - In 1D, this says: if moving from \\(x\\) to \\(y\\) decreases \\(f\\), then the derivative at \\(x\\) must not \u201cpoint uphill\u201d toward \\(y\\). - In higher-D, \\(\\nabla f(x)\\) gives the direction of steepest increase. So if \\(y\\) has lower value, the vector to \\(y\\) must make an obtuse angle with \\(\\nabla f(x)\\).</p> <p>Not true for arbitrary differentiable functions. It encodes: \u201csublevel sets are convex so gradients support them.\u201d</p> <p>Also: gradient is not \\(\\frac{f(y)-f(x)}{y-x}\\) in \\(\\mathbb{R}^n\\). That\u2019s a finite difference slope along a direction. Gradient is the vector of partial derivatives and satisfies: [ f(x+tv)=f(x)+t\\nabla f(x)^Tv+o(t). ]</p>"},{"location":"Convex_sets_and_functions/#part-c-schur-complement-where-it-comes-from","title":"PART C \u2014 Schur complement (where it comes from)","text":""},{"location":"Convex_sets_and_functions/#9-quadratic-in-xy-and-minimizing-out-y","title":"9) Quadratic in \\((x,y)\\) and minimizing out \\(y\\)","text":"<p>Let [ f(x,y)=x^TAx + 2x^TBy + y^TCy, \\quad \\begin{bmatrix}A &amp; B\\ B^T &amp; C\\end{bmatrix}\\succeq 0,\\ C\\succ0. ]</p> <p>Fix \\(x\\). Minimize over \\(y\\): - This is a convex quadratic in \\(y\\) since \\(C\\succ0\\). - First-order condition:   [   \\nabla_y f = 2B^Tx + 2Cy = 0   \\Rightarrow y^\\star = -C^{-1}B^Tx.   ] Plug back: [ g(x)=\\inf_y f(x,y) = x^TAx - 2x^TB C^{-1}B^T x + x^T B C^{-1} C C^{-1}B^T x = x^T(A-BC^{-1}B^T)x. ]</p> <p>Now the key logic: - If the block matrix is PSD and \\(C\\succ0\\), then for every \\(x\\),   [   g(x)=\\inf_y f(x,y)\\ge 0.   ] - But \\(g(x)=x^T(A-BC^{-1}B^T)x\\ge 0\\ \\forall x\\) holds iff   [   A-BC^{-1}B^T\\succeq 0.   ] That matrix \\(A-BC^{-1}B^T\\) is the Schur complement of \\(C\\).</p>"},{"location":"Convex_sets_and_functions/#part-d-conjugates-the-support-function-viewpoint","title":"PART D \u2014 Conjugates (the \u201csupport function\u201d viewpoint)","text":""},{"location":"Convex_sets_and_functions/#10-definition-what-dom-f-actually-means","title":"10) Definition + what \u201cdom \\(f^\\*\\)\u201d actually means","text":"\\[ f^\\*(y)=\\sup_{x\\in\\mathrm{dom}f}(y^Tx-f(x)). \\] <ul> <li>For each fixed \\(x\\), the map \\(y\\mapsto y^Tx-f(x)\\) is affine in \\(y\\).</li> <li>Sup of affine functions is convex \\(\\Rightarrow f^\\*\\) always convex (even if \\(f\\) isn\u2019t).</li> </ul> <p>Domain [ \\mathrm{dom}f^*={y\\mid \\sup_x(y^Tx-f(x))&lt;\\infty}. ] If for some \\(y\\) the expression can blow up to \\(+\\infty\\), then \\(f^\\*(y)=+\\infty\\) and that \\(y\\notin\\mathrm{dom}f^\\*\\).</p> <p>That\u2019s why some examples just report \u201c\\(\\mathrm{dom}f^\\*=\\mathbb{R}_+\\)\u201d etc: they\u2019re saying \u201coutside this set the value is \\(+\\infty\\)\u201d.</p>"},{"location":"Convex_sets_and_functions/#11-conjugate-of-max-function-exercise-336a","title":"11) Conjugate of max function (exercise 3.36a)","text":"<p>Let \\(f(x)=\\max_{i=1,\\dots,n} x_i\\) on \\(\\mathbb{R}^n\\).</p> <p>Compute: [ f^*(y)=\\sup_x\\left(y^Tx-\\max_i x_i\\right). ]</p> <p>Key trick: use the representation [ \\max_i x_i = \\sup_{\\lambda\\in\\Delta}\\ \\lambda^T x, \\qquad \\Delta={\\lambda\\ge0,\\ \\mathbf{1}^T\\lambda=1}. ] So [ y^Tx-\\max_i x_i = y^Tx-\\sup_{\\lambda\\in\\Delta}\\lambda^Tx = \\inf_{\\lambda\\in\\Delta}(y-\\lambda)^Tx. ]</p> <p>Now: - If \\(y\\notin\\Delta\\), we can pick a direction in \\(x\\) that makes \\((y-\\lambda)^Tx\\) arbitrarily large \u2192 supremum is \\(+\\infty\\). - If \\(y\\in\\Delta\\), choose \\(\\lambda=y\\), then \\((y-\\lambda)^Tx=0\\) for all \\(x\\), so the supremum is \\(0\\).</p> <p>Therefore [ f^*(y)= \\begin{cases} 0, &amp; y\\in\\Delta\\ (\\text{i.e. } y\\ge0,\\ \\mathbf{1}^Ty=1),\\ +\\infty, &amp; \\text{otherwise}. \\end{cases} ] So it\u2019s the indicator of the simplex.</p>"},{"location":"Convex_sets_and_functions/#12-power-function-conjugate-exercise-336d","title":"12) Power function conjugate (exercise 3.36d)","text":""},{"location":"Convex_sets_and_functions/#case-1-fxxp-on-mathbbr_-p1","title":"Case 1: \\(f(x)=x^p\\) on \\(\\mathbb{R}_{++}\\), \\(p&gt;1\\)","text":"\\[ f^\\*(y)=\\sup_{x&gt;0}(yx-x^p). \\] <p>If \\(y\\le 0\\): the best is to push \\(x\\to0^+\\), giving supremum \\(0\\). If \\(y&gt;0\\): optimize derivative: [ \\frac{d}{dx}(yx-x^p)=y-px^{p-1}=0 \\Rightarrow x^*=\\left(\\frac{y}{p}\\right)^{\\frac{1}{p-1}}. ] Plug in: [ f^*(y)=y\\left(\\frac{y}{p}\\right)^{\\frac{1}{p-1}} -\\left(\\frac{y}{p}\\right)^{\\frac{p}{p-1}} = \\left(\\frac{p-1}{p^{\\frac{p}{p-1}}}\\right)y^{\\frac{p}{p-1}}, \\quad y&gt;0. ]</p> <p>So [ f^*(y)= \\begin{cases} 0,&amp; y\\le 0,\\[4pt] \\displaystyle \\frac{p-1}{p^{p/(p-1)}}\\,y^{p/(p-1)},&amp; y&gt;0. \\end{cases} ]</p>"},{"location":"Convex_sets_and_functions/#case-2-p0-on-mathbbr_","title":"Case 2: \\(p&lt;0\\) on \\(\\mathbb{R}_{++}\\)","text":"<p>Then \\(x^p\\to +\\infty\\) as \\(x\\to0^+\\) and \\(x^p\\to0\\) as \\(x\\to\\infty\\). You handle it the same way: - For \\(y&gt;0\\): \\(yx-x^p\\to+\\infty\\) as \\(x\\to\\infty\\) \u21d2 \\(f^\\*(y)=+\\infty\\). - For \\(y&lt;0\\): interior maximizer exists; solve \\(y-px^{p-1}=0\\) again. You get a finite expression on \\(y&lt;0\\) and \\(+\\infty\\) otherwise.</p> <p>(When I do this fast, I just check the tails first to know where it\u2019s finite.)</p>"},{"location":"Convex_sets_and_functions/#part-e-duality-kkt-the-part-you-actually-solve-problems-with","title":"PART E \u2014 Duality + KKT (the part you actually solve problems with)","text":""},{"location":"Convex_sets_and_functions/#13-lagrangian-dual-function","title":"13) Lagrangian + dual function","text":"<p>Primal: [ \\min_x\\ f_0(x)\\quad\\text{s.t.}\\quad f_i(x)\\le0,\\ h_j(x)=0. ]</p> <p>Lagrangian: [ L(x,\\lambda,\\nu)=f_0(x)+\\sum_{i=1}^m\\lambda_i f_i(x)+\\sum_{j=1}^p\\nu_j h_j(x), \\quad \\lambda\\ge0. ]</p> <p>Dual function: [ g(\\lambda,\\nu)=\\inf_x L(x,\\lambda,\\nu). ]</p> <p>Weak duality: [ g(\\lambda,\\nu)\\le p^\\star\\quad\\forall\\lambda\\ge0,\\nu. ]</p> <p>Dual problem: [ \\max_{\\lambda\\ge0,\\nu} g(\\lambda,\\nu). ]</p> <p>Why dual is always \u201cconvex in dual vars\u201d - For fixed \\(x\\), \\(L\\) is affine in \\((\\lambda,\\nu)\\). - Infimum of affine functions is concave \u2192 \\(g\\) concave. - Maximizing concave over convex set (\\(\\lambda\\ge0\\)) = convex optimization in \\((\\lambda,\\nu)\\).</p>"},{"location":"Convex_sets_and_functions/#14-kkt-convex-case-slater","title":"14) KKT (convex case + Slater)","text":"<p>Assume: - \\(f_0,f_i\\) convex - \\(h_j\\) affine - Slater holds</p> <p>Then \\(x^\\*,\\lambda^\\*,\\nu^\\*\\) optimal iff:</p> <p>Primal feasibility: \\(f_i(x^\\*)\\le0,\\ h(x^\\*)=0\\)</p> <p>Dual feasibility: \\(\\lambda^\\*\\ge0\\)</p> <p>Stationarity: [ \\nabla f_0(x^*)+\\sum_i\\lambda_i^*\\nabla f_i(x^*)+\\sum_j\\nu_j^*\\nabla h_j(x^*)=0 ]</p> <p>Complementary slackness: [ \\lambda_i^* f_i(x^*)=0\\ \\ \\forall i. ]</p> <p>Mental picture: - Only active constraints can have nonzero multipliers. - Objective gradient is balanced by active constraint normals.</p>"},{"location":"Convex_sets_and_functions/#part-f-probability-simplex-sets-convexity-checklist","title":"PART F \u2014 Probability simplex sets (convexity checklist)","text":"<p>Let \\(p\\in\\mathbb{R}^n\\), \\(p\\ge0\\), \\(\\mathbf{1}^Tp=1\\). Random variable \\(x\\) takes values \\(a_i\\) with \\(\\Pr(x=a_i)=p_i\\).</p> <p>Common move: - Any expectation \\(\\mathbb{E}[\\phi(x)]=\\sum_i p_i\\phi(a_i)\\) is affine in \\(p\\).</p> <p>So constraints like \\(\\alpha\\le \\mathbb{E}[\\phi(x)]\\le\\beta\\) are just halfspaces in \\(p\\).</p> <p>Variance: [ \\mathrm{var}(x)=\\mathbb{E}[x^2]-(\\mathbb{E}[x])^2. ] Affine minus convex quadratic \u2192 concave in \\(p\\). So: - \\(\\mathrm{var}(x)\\le \\alpha\\): sublevel of concave \u2192 generally not convex. - \\(\\mathrm{var}(x)\\ge \\alpha\\): superlevel of concave \u2192 convex.</p> <p>Quantile: [ \\mathrm{quartile}(x)=\\inf{\\beta\\mid \\Pr(x\\le \\beta)\\ge 0.25}. ] For discrete \\(a_i\\), \\(\\Pr(x\\le \\beta)=\\sum_{i:a_i\\le \\beta} p_i\\) is affine in \\(p\\), so conditions of the form \u201cquartile \\(\\le\\) / \\(\\ge\\)\u201d become intersections of halfspaces.</p>"},{"location":"Convex_sets_and_functions/#part-g-approximation-width-exercise-342-flavor","title":"PART G \u2014 Approximation width (exercise 3.42 flavor)","text":"<p>Given fixed \\(\\epsilon&gt;0\\), [ W(x)=\\sup{T\\mid |x_1f_1(t)+\\cdots+x_nf_n(t)-f_0(t)|\\le\\epsilon,\\ 0\\le t\\le T}. ]</p> <p>For a fixed \\(t\\), define \\(a(t)=(f_1(t),\\dots,f_n(t))\\). Then [ |a(t)^Tx-f_0(t)|\\le \\epsilon \\iff \\begin{cases} a(t)^Tx \\le f_0(t)+\\epsilon\\ -a(t)^Tx \\le \\epsilon-f_0(t) \\end{cases} ] (two halfspaces in \\(x\\)).</p> <p>Superlevel set of \\(W\\): [ {x\\mid W(x)\\ge T} = \\bigcap_{t\\in[0,T]} {x\\mid |a(t)^Tx-f_0(t)|\\le\\epsilon}. ] Intersection of convex sets \u2192 convex. So \\(W\\) is quasiconcave.</p> <p>(Intuition: \u201chaving width at least \\(T\\)\u201d is a robust feasibility condition over all \\(t\\le T\\).)</p>"},{"location":"Strong_convexity/","title":"Strong Convexity + Smoothness (\u03b2-smooth) \u2014 Cheat Sheet","text":"<p>Let \\(f:\\mathbb{R}^n\\to\\mathbb{R}\\) be differentiable (unless stated otherwise). We write \\(\\|\\cdot\\|\\) for the Euclidean norm.</p>"},{"location":"Strong_convexity/#0-the-one-line-picture-what-these-properties-mean","title":"0) The one-line picture (what these properties mean)","text":"<ul> <li>Strong convexity (\\(\\alpha\\)): \\(f\\) has curvature at least \\(\\alpha\\) everywhere \u2192 it grows at least quadratically away from minimizers.</li> <li>Smoothness (\\(\\beta\\)): \\(f\\) has curvature at most \\(\\beta\\) everywhere \u2192 it is never \u201ctoo curved\u201d; gradients don\u2019t change too fast.</li> </ul> <p>If both hold, \\(f\\) is \u201csandwiched\u201d between two quadratics.</p>"},{"location":"Strong_convexity/#1-correct-first-order-inequalities-the-ones-i-always-use","title":"1) Correct first-order inequalities (the ones I always use)","text":""},{"location":"Strong_convexity/#a-alpha-strong-convexity-quadratic-lower-bound","title":"(A) \\(\\alpha\\)-strong convexity (quadratic lower bound)","text":"<p>\\(f\\) is \\(\\alpha\\)-strongly convex iff for all \\(x,y\\), $$ f(y)\\ \\ge\\ f(x)+\\langle \\nabla f(x),y-x\\rangle+\\frac{\\alpha}{2}|y-x|^2. $$</p>"},{"location":"Strong_convexity/#b-beta-smoothness-quadratic-upper-bound-descent-lemma","title":"(B) \\(\\beta\\)-smoothness (quadratic upper bound / descent lemma)","text":"<p>\\(f\\) is \\(\\beta\\)-smooth iff for all \\(x,y\\), $$ f(y)\\ \\le\\ f(x)+\\langle \\nabla f(x),y-x\\rangle+\\frac{\\beta}{2}|y-x|^2. $$</p> <p>Common mistake I watch for: the inequality directions are opposite. Strong convexity gives a lower quadratic support; smoothness gives an upper quadratic envelope.</p>"},{"location":"Strong_convexity/#2-the-sandwich-when-both-hold","title":"2) The \u201csandwich\u201d when both hold","text":"<p>If \\(f\\) is both \\(\\alpha\\)-strongly convex and \\(\\beta\\)-smooth, then for all \\(x,y\\), $$ f(x)+\\langle \\nabla f(x),y-x\\rangle+\\frac{\\alpha}{2}|y-x|^2 \\ \\le\\ f(y)\\ \\le\\ f(x)+\\langle \\nabla f(x),y-x\\rangle+\\frac{\\beta}{2}|y-x|^2. $$</p> <p>So the same linearization sits in between two quadratics.</p> <p>No, it does NOT have to hold with equality. Equality for both bounds for all \\(x,y\\) essentially forces \\(f\\) to be an exact quadratic (details in \u00a78).</p>"},{"location":"Strong_convexity/#3-equivalent-gradient-conditions-super-useful","title":"3) Equivalent gradient conditions (super useful)","text":""},{"location":"Strong_convexity/#a-beta-smooth-lipschitz-gradient","title":"(A) \\(\\beta\\)-smooth  \u21d4  Lipschitz gradient","text":"\\[ \\|\\nabla f(x)-\\nabla f(y)\\|\\ \\le\\ \\beta\\|x-y\\|\\quad\\forall x,y. \\] <p>Also equivalent (Baillon\u2013Haddad / cocoercivity for convex \\(f\\)): $$ \\langle \\nabla f(x)-\\nabla f(y),x-y\\rangle\\ \\ge\\ \\frac{1}{\\beta}|\\nabla f(x)-\\nabla f(y)|^2. $$</p>"},{"location":"Strong_convexity/#b-alpha-strongly-convex-strongly-monotone-gradient","title":"(B) \\(\\alpha\\)-strongly convex  \u21d4  strongly monotone gradient","text":"\\[ \\langle \\nabla f(x)-\\nabla f(y),x-y\\rangle\\ \\ge\\ \\alpha\\|x-y\\|^2\\quad\\forall x,y. \\]"},{"location":"Strong_convexity/#c-if-both-hold","title":"(C) If both hold","text":"<p>Combine them: $$ \\alpha|x-y|^2\\ \\le\\ \\langle \\nabla f(x)-\\nabla f(y),x-y\\rangle\\ \\le\\ \\beta|x-y|^2. $$</p>"},{"location":"Strong_convexity/#4-if-f-is-twice-differentiable-hessian-criteria-the-cleanest-test","title":"4) If \\(f\\) is twice differentiable: Hessian criteria (the cleanest test)","text":"<p>If \\(f\\in C^2\\), then:</p> <ul> <li> <p>\\(\\beta\\)-smooth  \u21d4  for all \\(x\\), $$ \\nabla^2 f(x)\\ \\preceq\\ \\beta I. $$</p> </li> <li> <p>\\(\\alpha\\)-strongly convex  \u21d4  for all \\(x\\), $$ \\nabla^2 f(x)\\ \\succeq\\ \\alpha I. $$</p> </li> </ul> <p>So both hold iff $$ \\alpha I\\ \\preceq\\ \\nabla^2 f(x)\\ \\preceq\\ \\beta I\\quad \\forall x. $$</p> <p>This is literally \u201ceigenvalues of Hessian stay in \\([\\alpha,\\beta]\\).\u201d</p>"},{"location":"Strong_convexity/#5-bregman-divergence-viewpoint-my-favorite-clean-bound","title":"5) Bregman divergence viewpoint (my favorite clean bound)","text":"<p>Define the Bregman divergence: $$ D_f(y,x)=f(y)-f(x)-\\langle \\nabla f(x),y-x\\rangle. $$</p> <p>Then: - \\(\\alpha\\)-strong convexity \u21d4 \\(D_f(y,x)\\ge \\frac{\\alpha}{2}\\|y-x\\|^2\\). - \\(\\beta\\)-smoothness \u21d4 \\(D_f(y,x)\\le \\frac{\\beta}{2}\\|y-x\\|^2\\).</p> <p>So if both hold: $$ \\frac{\\alpha}{2}|y-x|^2\\ \\le\\ D_f(y,x)\\ \\le\\ \\frac{\\beta}{2}|y-x|^2. $$</p>"},{"location":"Strong_convexity/#6-consequences-around-the-minimizer-xstar","title":"6) Consequences around the minimizer \\(x^\\star\\)","text":"<p>Assume \\(f\\) is convex and differentiable and has minimizer \\(x^\\star\\) with \\(\\nabla f(x^\\star)=0\\).</p>"},{"location":"Strong_convexity/#a-quadratic-growth-from-strong-convexity","title":"(A) Quadratic growth (from strong convexity)","text":"\\[ f(x)-f(x^\\star)\\ \\ge\\ \\frac{\\alpha}{2}\\|x-x^\\star\\|^2. \\]"},{"location":"Strong_convexity/#b-upper-bound-on-suboptimality-from-smoothness","title":"(B) Upper bound on suboptimality (from smoothness)","text":"<p>Using smoothness with \\(y=x^\\star\\): $$ f(x)-f(x^\\star)\\ \\le\\ \\frac{\\beta}{2}|x-x^\\star|^2. $$</p>"},{"location":"Strong_convexity/#c-gradient-norm-bounds-when-both-hold","title":"(C) Gradient norm bounds (when both hold)","text":"<p>From strong monotonicity + Lipschitz: $$ \\alpha|x-x^\\star|\\ \\le\\ |\\nabla f(x)|\\ \\le\\ \\beta|x-x^\\star|. $$</p> <p>Also a classic PL-type bound holds for \\(\\alpha\\)-strongly convex + \\(\\beta\\)-smooth: $$ f(x)-f(x^\\star)\\ \\le\\ \\frac{1}{2\\alpha}|\\nabla f(x)|^2 \\quad\\text{and}\\quad |\\nabla f(x)|^2\\ \\ge\\ 2\\alpha\\,(f(x)-f(x^\\star)). $$</p> <p>(These are the inequalities that make GD linear.)</p>"},{"location":"Strong_convexity/#7-gradient-descent-step-sizes-rate-why-we-care","title":"7) Gradient descent: step sizes + rate (why we care)","text":"<p>Gradient descent: \\(x_{k+1}=x_k-\\eta \\nabla f(x_k)\\).</p> <p>If \\(f\\) is \\(\\beta\\)-smooth and convex, choosing \\(0&lt;\\eta\\le 1/\\beta\\) gives descent.</p> <p>If \\(f\\) is \\(\\alpha\\)-strongly convex and \\(\\beta\\)-smooth, then with \\(\\eta=1/\\beta\\), $$ f(x_k)-f(x^\\star)\\ \\le\\ \\left(1-\\frac{\\alpha}{\\beta}\\right)^k \\big(f(x_0)-f(x^\\star)\\big). $$</p> <p>Condition number: \\(\\kappa=\\beta/\\alpha\\). Smaller \\(\\kappa\\) \u2192 faster.</p>"},{"location":"Strong_convexity/#8-your-question-if-both-hold-does-it-have-to-be-equality","title":"8) Your question: \u201cIf both hold, does it have to be equality?\u201d","text":"<p>No. The definition only requires the inequalities. In general they\u2019re strict for many \\(x\\neq y\\).</p>"},{"location":"Strong_convexity/#when-can-equality-happen","title":"When can equality happen?","text":"<ul> <li>If equality holds in the smoothness upper bound for all \\(x,y\\), \\(f\\) must behave like a quadratic with curvature \\(\\beta\\).</li> <li>If equality holds in the strong convexity lower bound for all \\(x,y\\), \\(f\\) must behave like a quadratic with curvature \\(\\alpha\\).</li> </ul> <p>For both to be equalities for all \\(x,y\\), you essentially need: - \\(\\alpha=\\beta\\), and - \\(f\\) is exactly a quadratic: $$ f(x)=\\frac{\\alpha}{2}|x|^2+\\langle b,x\\rangle+c $$ (up to the usual affine/constant terms).</p> <p>So: equality is the \u201cpure quadratic\u201d extreme case, not the general case.</p>"},{"location":"Strong_convexity/#9-quick-examples-to-sanity-check","title":"9) Quick examples (to sanity check)","text":""},{"location":"Strong_convexity/#example-1-quadratic-tight-equality-type-behavior","title":"Example 1: Quadratic (tight / equality-type behavior)","text":"<p>Let \\(f(x)=\\frac{1}{2}x^\\top Qx\\) with \\(Q\\succeq 0\\). If eigenvalues of \\(Q\\) lie in \\([\\alpha,\\beta]\\), then \\(f\\) is \\(\\alpha\\)-strongly convex and \\(\\beta\\)-smooth. If \\(Q=\\alpha I\\), then \\(\\alpha=\\beta\\) and the bounds are maximally tight.</p>"},{"location":"Strong_convexity/#example-2-strongly-convex-smooth-but-not-equality","title":"Example 2: Strongly convex + smooth but not equality","text":"<p>\\(f(x)=\\frac{1}{2}\\|x\\|^2 + 0.01\\sum_i \\cos(x_i)\\) on regions where Hessian stays bounded between two constants. The inequalities hold, but you don\u2019t get equality except maybe at special points/directions.</p>"},{"location":"Strong_convexity/#10-mini-checklist-how-i-verify-fast","title":"10) Mini checklist (how I verify fast)","text":"<p>1) If I can compute Hessian: check \\(\\alpha I\\preceq \\nabla^2 f(x)\\preceq \\beta I\\). 2) If not: try gradient tests:    - smooth: \\(\\|\\nabla f(x)-\\nabla f(y)\\|\\le \\beta\\|x-y\\|\\)    - strong: \\(\\langle \\nabla f(x)-\\nabla f(y),x-y\\rangle \\ge \\alpha\\|x-y\\|^2\\) 3) Remember the sandwich: lower bound uses \\(\\alpha\\), upper bound uses \\(\\beta\\).</p>"},{"location":"cheatsheet_norm/","title":"Norms + Dual Norms in Optimization \u2014 Cheat Sheet","text":"<p>Let \\(\\|\\cdot\\|\\) be a norm on \\(\\mathbb{R}^n\\) and \\(\\|\\cdot\\|_*\\) its dual norm.</p>"},{"location":"cheatsheet_norm/#1-norm-axioms-immediate-consequences","title":"1) Norm axioms + immediate consequences","text":"<p>Axioms - Positive homogeneity: \\(\\|\\alpha x\\| = |\\alpha|\\,\\|x\\|\\) - Triangle inequality: \\(\\|x+y\\| \\le \\|x\\|+\\|y\\|\\) - Definiteness: \\(\\|x\\|=0 \\iff x=0\\)</p> <p>Consequences - Reverse triangle: \\(\\big|\\|x\\|-\\|y\\|\\big| \\le \\|x-y\\|\\) - Convexity: for \\(\\theta\\in[0,1]\\),</p> <p>$$   |\\theta x+(1-\\theta)y| \\le \\theta|x|+(1-\\theta)|y|   $$</p> <ul> <li>Balls scale: \\(\\{x:\\|x\\|\\le r\\} = r\\,\\{x:\\|x\\|\\le 1\\}\\)</li> </ul>"},{"location":"cheatsheet_norm/#2-dual-norm-definitions-core-inequalities","title":"2) Dual norm definitions + core inequalities","text":"<p>Dual norm</p> \\[ \\|y\\|_* = \\sup_{\\|x\\|\\le 1} y^\\top x \\;=\\; \\sup_{\\|x\\|=1} y^\\top x \\] <p>General H\u00f6lder inequality</p> \\[ y^\\top x \\le \\|x\\|\\,\\|y\\|_* \\quad \\forall x,y \\] <p>Bipolar / dual-of-dual representation</p> \\[ \\|x\\| = \\sup_{\\|y\\|_*\\le 1} y^\\top x \\] <p>Max over a ball</p> \\[ \\sup_{\\|x\\|\\le r} y^\\top x = r\\,\\|y\\|_* \\]"},{"location":"cheatsheet_norm/#3-dual-cones-balls-and-support-functions","title":"3) Dual cones, balls, and support functions","text":"<p>Primal unit ball</p> \\[ B = \\{x:\\|x\\|\\le 1\\} \\] <p>Dual unit ball</p> \\[ B_* = \\{y:\\|y\\|_*\\le 1\\} \\] <p>Support function view</p> \\[ \\sigma_B(y) = \\sup_{x\\in B} y^\\top x = \\|y\\|_* \\]"},{"location":"cheatsheet_norm/#4-conjugates-fenchel-the-norm-linear-pattern","title":"4) Conjugates (Fenchel) \u2014 the \u201cnorm \u2212 linear\u201d pattern","text":"<p>For \\(f(x)=\\|x\\|\\), its conjugate is</p> \\[ f^*(y)=\\sup_x \\left(y^\\top x-\\|x\\|\\right) = \\begin{cases} 0, &amp; \\|y\\|_*\\le 1,\\\\ +\\infty, &amp; \\text{otherwise.} \\end{cases} \\] <p>Equivalently,</p> \\[ \\inf_x \\left(\\|x\\|-y^\\top x\\right) = \\begin{cases} 0, &amp; \\|y\\|_*\\le 1,\\\\ -\\infty, &amp; \\text{otherwise.} \\end{cases} \\] <p>Indicator connection. Let \\(\\delta_{B_*}(y)=0\\) if \\(y\\in B_*\\) and \\(+\\infty\\) otherwise. Then \\(f^*(y)=\\delta_{B_*}(y)\\).</p>"},{"location":"cheatsheet_norm/#5-subgradients-of-a-norm-kkt-workhorse","title":"5) Subgradients of a norm (KKT workhorse)","text":"<p>Characterization</p> \\[ g\\in \\partial\\|x\\| \\iff \\|g\\|_*\\le 1 \\ \\text{and}\\ g^\\top x = \\|x\\| \\] <p>At the origin</p> \\[ \\partial\\|0\\| = \\{g:\\|g\\|_*\\le 1\\} = B_* \\]"},{"location":"cheatsheet_norm/#6-common-dual-pairs-memorize","title":"6) Common dual pairs (memorize)","text":"<ul> <li>\\(\\|x\\|_2 \\leftrightarrow \\|y\\|_2\\)</li> <li>\\(\\|x\\|_1 \\leftrightarrow \\|y\\|_\\infty\\)</li> <li>\\(\\|x\\|_\\infty \\leftrightarrow \\|y\\|_1\\)</li> </ul> <p>Weighted 2-norm: if \\(W\\succ 0\\) and</p> \\[ \\|x\\|_W = \\sqrt{x^\\top W x}, \\] <p>then</p> \\[ \\|y\\|_{W,*} = \\sqrt{y^\\top W^{-1} y}. \\]"},{"location":"cheatsheet_norm/#7-modeling-epigraphs-lpsocp-forms","title":"7) Modeling epigraphs (LP/SOCP forms)","text":"<p>2-norm (SOC)</p> \\[ \\|x\\|_2 \\le t \\iff (t,x)\\in \\mathcal{Q}^{n+1} \\] <p>1-norm (LP)</p> \\[ \\|x\\|_1 \\le t \\iff \\exists u\\ge 0:\\ -u\\le x\\le u,\\ \\mathbf{1}^\\top u \\le t \\] <p>Infinity-norm (LP)</p> \\[ \\|x\\|_\\infty \\le t \\iff -t\\mathbf{1}\\le x\\le t\\mathbf{1} \\] <p>Quadratic form (SOC via factorization) If \\(P\\succeq 0\\) and \\(P=L^\\top L\\), then</p> \\[ x^\\top P x \\le t^2 \\iff \\|Lx\\|_2 \\le t \\]"},{"location":"cheatsheet_norm/#8-distance-to-hyperplaneshalfspaces-in-a-norm-uses-dual-norm","title":"8) Distance to hyperplanes/halfspaces in a norm (uses dual norm)","text":"<p>Hyperplane \\(H=\\{z:a^\\top z=b\\}\\):</p> \\[ \\mathrm{dist}_{\\|\\cdot\\|}(x,H) = \\frac{|a^\\top x-b|}{\\|a\\|_*} \\] <p>Halfspace \\(\\{z:a^\\top z\\le b\\}\\):</p> \\[ \\mathrm{dist}_{\\|\\cdot\\|}(x,\\{a^\\top z\\le b\\}) = \\frac{\\max(0,a^\\top x-b)}{\\|a\\|_*} \\]"},{"location":"cheatsheet_norm/#9-equality-constrained-norm-minimization-dual-classic-boyd-result","title":"9) Equality-constrained norm minimization dual (classic Boyd result)","text":"<p>Primal:</p> \\[ \\min_x \\|x\\|\\quad \\text{s.t.}\\quad Ax=b \\] <p>Dual:</p> \\[ \\max_\\nu b^\\top \\nu \\quad \\text{s.t.}\\quad \\|A^\\top \\nu\\|_* \\le 1 \\] <p>Lower bound property:</p> \\[ p^\\star \\ge b^\\top \\nu \\quad \\text{for any }\\nu\\text{ with }\\|A^\\top \\nu\\|_* \\le 1 \\]"},{"location":"cheatsheet_norm/#10-quick-translation-rules-mental-toolbox","title":"10) Quick \u201ctranslation rules\u201d (mental toolbox)","text":"<ul> <li>\\(\\|y\\|_* = \\sup_{\\|x\\|\\le 1} y^\\top x\\) (support function)</li> <li>\\(y^\\top x \\le \\|x\\|\\,\\|y\\|_*\\) (H\u00f6lder)</li> <li>Conjugate of a norm is an indicator of the dual ball</li> <li>Subgradient of a norm is a dual-ball vector that attains equality</li> </ul>"},{"location":"convexity_preserving_operations/","title":"Convex Sets + Mappings + Cones + Perspective \u2014 Cheat Sheet","text":"<p>This sheet collects a few \u201cconvexity-preserving operations\u201d that show up everywhere in convex optimization: - projections - affine images + preimages - perspective / linear-fractional ideas - proper cones + generalized inequalities - minimum vs minimal elements - distance-to-set regions (and why they can fail convexity)</p>"},{"location":"convexity_preserving_operations/#1-projection-of-a-convex-set-is-convex","title":"1) Projection of a convex set is convex","text":"<p>Let \\(S \\subseteq \\mathbb{R}^m \\times \\mathbb{R}^n\\) be convex. Define the projection onto the first block: [ T={x_1\\in\\mathbb{R}^m\\mid \\exists x_2\\in\\mathbb{R}^n \\text{ s.t. } (x_1,x_2)\\in S}. ]</p> <p>Claim. \\(T\\) is convex.</p> <p>Proof (one-liner style). Take \\(u_1,v_1\\in T\\). Then \\(\\exists u_2,v_2\\) with \\((u_1,u_2),(v_1,v_2)\\in S\\). For \\(\\theta\\in[0,1]\\), [ (\\theta u_1+(1-\\theta)v_1,\\ \\theta u_2+(1-\\theta)v_2) = \\theta(u_1,u_2)+(1-\\theta)(v_1,v_2)\\in S ] (by convexity of \\(S\\)). Hence the first coordinate \\(\\theta u_1+(1-\\theta)v_1\\in T\\). \u2705</p> <p>Mental picture. Projection \u201cdrops coordinates.\u201d Convexity survives because you can convex-combine witnesses.</p>"},{"location":"convexity_preserving_operations/#2-affine-maps-image-inverse-image-preserve-convexity","title":"2) Affine maps: image + inverse image preserve convexity","text":""},{"location":"convexity_preserving_operations/#21-affine-function","title":"2.1 Affine function","text":"<p>A function \\(f:\\mathbb{R}^n\\to\\mathbb{R}^m\\) is affine if [ f(x)=Ax+b. ]</p>"},{"location":"convexity_preserving_operations/#22-image-of-a-convex-set-under-affine-map","title":"2.2 Image of a convex set under affine map","text":"<p>Let \\(S\\subseteq\\mathbb{R}^n\\) be convex. Define [ f(S)={f(x)\\mid x\\in S}. ]</p> <p>Claim. \\(f(S)\\) is convex.</p> <p>Reason. If \\(y_1=f(x_1)\\), \\(y_2=f(x_2)\\), then for \\(\\theta\\in[0,1]\\), [ \\theta y_1+(1-\\theta)y_2 = \\theta f(x_1)+(1-\\theta)f(x_2) = f(\\theta x_1+(1-\\theta)x_2)\\in f(S) ] since \\(\\theta x_1+(1-\\theta)x_2\\in S\\).</p>"},{"location":"convexity_preserving_operations/#23-inverse-image-preimage-of-a-convex-set-under-affine-map","title":"2.3 Inverse image (preimage) of a convex set under affine map","text":"<p>Let \\(C\\subseteq\\mathbb{R}^m\\) be convex. Define [ f^{-1}(C)={x\\in\\mathbb{R}^n\\mid f(x)\\in C}. ]</p> <p>Claim. \\(f^{-1}(C)\\) is convex.</p> <p>Reason. If \\(x_1,x_2\\in f^{-1}(C)\\) then \\(f(x_1),f(x_2)\\in C\\). For \\(\\theta\\in[0,1]\\), [ f(\\theta x_1+(1-\\theta)x_2)=\\theta f(x_1)+(1-\\theta)f(x_2)\\in C, ] so \\(\\theta x_1+(1-\\theta)x_2\\in f^{-1}(C)\\).</p> <p>Big takeaway. - affine image preserves convexity - affine preimage preserves convexity - projection is a special case of affine image (drop coordinates)</p>"},{"location":"convexity_preserving_operations/#3-inverse-function-vs-inverse-image-preimage-dont-mix","title":"3) Inverse function vs inverse image (preimage) \u2014 don\u2019t mix","text":"<p>Let \\(f:X\\to Y\\).</p> <p>Inverse function \\(f^{-1}:Y\\to X\\) exists only if \\(f\\) is bijective (on the sets considered): [ f^{-1}(y)=x \\iff f(x)=y. ]</p> <p>Inverse image / preimage works for any \\(f\\): [ f^{-1}(B)={x\\in X \\mid f(x)\\in B},\\quad B\\subseteq Y. ]</p> <p>Same symbol \\(f^{-1}\\), different meaning: - \\(f^{-1}(y)\\) (point input) = inverse function (if it exists) - \\(f^{-1}(B)\\) (set input) = preimage (always exists)</p>"},{"location":"convexity_preserving_operations/#4-perspective-function-and-why-it-preserves-convexity","title":"4) Perspective function (and why it preserves convexity)","text":"<p>Define \\(P:\\mathbb{R}^{n+1}\\to\\mathbb{R}^n\\) by [ P(z,t)=\\frac{z}{t},\\qquad \\text{dom }P=\\mathbb{R}^n\\times \\mathbb{R}_{++}. ]</p>"},{"location":"convexity_preserving_operations/#41-line-segments-map-to-line-segments-reparameterized","title":"4.1 Line segments map to line segments (reparameterized)","text":"<p>Take \\(x=(\\tilde x,x_{n+1})\\), \\(y=(\\tilde y,y_{n+1})\\) with \\(x_{n+1},y_{n+1}&gt;0\\). For \\(\\theta\\in[0,1]\\), [ P(\\theta x+(1-\\theta)y) =\\frac{\\theta \\tilde x+(1-\\theta)\\tilde y}{\\theta x_{n+1}+(1-\\theta)y_{n+1}}. ]</p> <p>This can be rewritten as [ P(\\theta x+(1-\\theta)y)=\\mu(\\theta)\\,P(x)+(1-\\mu(\\theta))\\,P(y), ] where [ \\mu(\\theta)=\\frac{\\theta x_{n+1}}{\\theta x_{n+1}+(1-\\theta)y_{n+1}}\\in[0,1]. ]</p> <p>What matters. As \\(\\theta\\) sweeps \\(0\\to1\\), \\(\\mu(\\theta)\\) sweeps \\(0\\to1\\) (monotone + continuous), so: [ P([x,y])=[P(x),P(y)]. ]</p>"},{"location":"convexity_preserving_operations/#42-convexity-of-pc","title":"4.2 Convexity of \\(P(C)\\)","text":"<p>If \\(C\\subseteq \\text{dom }P\\) is convex, then for any \\(x,y\\in C\\), \\([x,y]\\subseteq C\\). Apply the segment mapping: [ [P(x),P(y)] = P([x,y]) \\subseteq P(C). ] So \\(P(C)\\) contains the whole segment between any two of its points \u21d2 \\(P(C)\\) is convex.</p>"},{"location":"convexity_preserving_operations/#43-convexity-of-the-preimage-under-perspective","title":"4.3 Convexity of the preimage under perspective","text":"<p>If \\(D\\subseteq \\mathbb{R}^n\\) is convex, then [ P^{-1}(D)={(z,t)\\in \\mathbb{R}^{n+1}\\mid z/t\\in D,\\ t&gt;0} ] is convex.</p> <p>Quick check. If \\((z_1,t_1),(z_2,t_2)\\in P^{-1}(D)\\), then \\(z_1/t_1, z_2/t_2\\in D\\). For \\(\\theta\\in[0,1]\\), [ \\frac{\\theta z_1+(1-\\theta)z_2}{\\theta t_1+(1-\\theta)t_2} = \\mu\\frac{z_1}{t_1}+(1-\\mu)\\frac{z_2}{t_2}\\in D ] for a suitable \\(\\mu\\in[0,1]\\) (same trick). Also \\(\\theta t_1+(1-\\theta)t_2&gt;0\\). Hence the convex combo stays in \\(P^{-1}(D)\\).</p> <p>Pin-hole camera intuition. Divide by depth \\(t\\) = \u201cproject to image plane.\u201d Convex objects stay convex after projection.</p>"},{"location":"convexity_preserving_operations/#5-proper-cones-generalized-inequalities","title":"5) Proper cones + generalized inequalities","text":"<p>A cone \\(K\\subseteq\\mathbb{R}^n\\) is proper if: - \\(K\\) is convex - \\(K\\) is closed - \\(K\\) is solid: \\(\\text{int }K\\neq \\emptyset\\) - \\(K\\) is pointed: \\(K\\cap(-K)=\\{0\\}\\) (contains no line)</p>"},{"location":"convexity_preserving_operations/#51-generalized-inequality-induced-by-k","title":"5.1 Generalized inequality induced by \\(K\\)","text":"<p>Define [ x \\preceq_K y \\iff y-x\\in K, \\qquad x \\prec_K y \\iff y-x\\in \\text{int }K. ]</p> <p>What \\(\\text{int }K\\) means. Interior of \\(K\\): [ \\text{int }K={u\\in K\\mid \\exists \\epsilon&gt;0: B(u,\\epsilon)\\subseteq K}. ] So strict inequality means \u201cdifference is strictly inside the cone.\u201d</p>"},{"location":"convexity_preserving_operations/#52-standard-examples","title":"5.2 Standard examples","text":"<ul> <li>\\(K=\\mathbb{R}^n_+\\): \\(x\\preceq_K y \\iff x_i\\le y_i\\ \\forall i\\). \\(\\text{int }K=\\{u: u_i&gt;0\\ \\forall i\\}\\) gives strict componentwise inequality.</li> <li>\\(K=S^n_+\\) (PSD cone): \\(X\\preceq_K Y \\iff Y-X\\succeq 0\\). \\(\\text{int }K\\) = PD matrices.</li> </ul>"},{"location":"convexity_preserving_operations/#6-minimum-vs-minimal-element-wrt-preceq_k","title":"6) Minimum vs minimal element (w.r.t. \\(\\preceq_K\\))","text":"<p>Let \\(S\\subseteq\\mathbb{R}^n\\).</p>"},{"location":"convexity_preserving_operations/#61-minimum-element","title":"6.1 Minimum element","text":"<p>\\(x\\in S\\) is a minimum element if [ \\forall y\\in S,\\quad x\\preceq_K y. ] If a minimum exists, it is unique.</p> <p>Set form. \\(x\\) is minimum iff [ S \\subseteq x+K. ] Because \\(y\\in x+K \\iff y-x\\in K \\iff x\\preceq_K y\\).</p>"},{"location":"convexity_preserving_operations/#62-minimal-element","title":"6.2 Minimal element","text":"<p>\\(x\\in S\\) is minimal if there is no strictly smaller different point: [ y\\in S,\\ y\\preceq_K x \\ \\Rightarrow\\ y=x. ] A set can have many minimal elements.</p> <p>Set form. [ (x-K)\\cap S={x}. ] Because \\(y\\in x-K \\iff x-y\\in K \\iff y\\preceq_K x\\).</p> <p>How to remember. - minimum = \u201cbeats everyone\u201d - minimal = \u201cnobody beats it (except itself)\u201d - minimum \u21d2 minimal, but not conversely</p>"},{"location":"convexity_preserving_operations/#7-distance-to-a-set-closer-to-s-than-t-region","title":"7) Distance to a set + \u201ccloser to S than T\u201d region","text":"<p>Distance-to-set: [ \\text{dist}(x,S)=\\inf_{z\\in S}|x-z|_2. ] (Inf is used because a closest point may not exist if \\(S\\) is not closed.)</p> <p>Region: [ \\Omega={x\\mid \\text{dist}(x,S)\\le \\text{dist}(x,T)}. ]</p>"},{"location":"convexity_preserving_operations/#71-important-fact-omega-is-not-convex-in-general","title":"7.1 Important fact: \\(\\Omega\\) is NOT convex in general","text":"<p>Even if \\(S\\) is convex and \\(T\\) is a point, \\(\\Omega\\) can be nonconvex.</p> <p>Why the \u201chalfspace intersection\u201d trick fails. - For a fixed point \\(s\\) and \\(t\\), the set \\(\\{x:\\|x-s\\|\\le \\|x-t\\|\\}\\) is a halfspace (convex). - But \\(\\text{dist}(x,S)\\le \\|x-t\\|\\) means:   [   \\inf_{s\\in S}|x-s|\\le |x-t|   ]   which behaves like \u201cthere exists an \\(s\\in S\\) that is close enough\u201d \u21d2 union, not intersection.</p> <p>If \\(S=\\{s_1,\\dots,s_k\\}\\) finite: [ \\min_i |x-s_i|\\le |x-t| \\iff \\bigcup_{i=1}^k {x:|x-s_i|\\le |x-t|}. ] Union of convex sets need not be convex.</p>"},{"location":"convexity_preserving_operations/#72-special-case-when-it-is-convex","title":"7.2 Special case when it IS convex","text":"<p>If \\(S\\) is a single point \\(S=\\{s\\}\\), then: [ |x-s|\\le \\text{dist}(x,T) \\iff \\forall t\\in T:\\ |x-s|\\le |x-t|, ] which is an intersection of halfspaces \u21d2 convex.</p> <p>Useful algebra (point vs point). [ |x-s|^2\\le |x-t|^2 \\iff 2(t-s)^\\top x \\le |t|^2-|s|^2. ] So it\u2019s literally a halfspace boundary (a perpendicular bisector hyperplane).</p>"},{"location":"convexity_preserving_operations/#8-quick-convexity-preserving-checklist","title":"8) Quick \u201cconvexity-preserving\u201d checklist","text":"<ul> <li>Projection of convex set \u2192 convex \u2705</li> <li>Affine image of convex set \u2192 convex \u2705</li> <li>Affine preimage of convex set \u2192 convex \u2705</li> <li>Perspective image of convex set (within dom) \u2192 convex \u2705</li> <li>Cone-induced order: use \\(K\\) + \\(\\text{int }K\\) for strict \u2705</li> <li>Minimum vs minimal: minimum unique, minimal possibly many \u2705</li> <li>\u201cCloser to set \\(S\\) than set \\(T\\)\u201d region \u2192 not convex in general \u274c (unless special structure like singleton)</li> </ul>"},{"location":"duality_concept/","title":"Lagrangian Duality + KKT + Geometric Intuition \u2014 Cheat Sheet","text":"<p>Let the primal problem be</p> \\[ \\min_x \\ f_0(x) \\quad \\text{s.t.}\\quad f_i(x)\\le 0,\\ i=1,\\dots,m,\\qquad h_j(x)=0,\\ j=1,\\dots,p. \\]"},{"location":"duality_concept/#1-why-lagrange-multipliers-exist-intuition","title":"1) Why Lagrange multipliers exist (intuition)","text":"<p>\u201cPrices for constraint violation.\u201d Introduce multipliers \\(\\lambda_i\\ge 0\\) for inequalities and \\(\\nu_j\\in\\mathbb{R}\\) for equalities, and combine constraints into the objective:</p> \\[ L(x,\\lambda,\\nu)=f_0(x)+\\sum_{i=1}^m \\lambda_i f_i(x)+\\sum_{j=1}^p \\nu_j h_j(x). \\] <ul> <li>If \\(x\\) is feasible, then \\(f_i(x)\\le 0\\) and with \\(\\lambda_i\\ge 0\\) we get \\(\\lambda_i f_i(x)\\le 0\\), so</li> </ul> <p>$$   L(x,\\lambda,\\nu)\\le f_0(x).   $$</p> <ul> <li>If \\(x\\) violates a constraint (\\(f_i(x)&gt;0\\)), the term \\(\\lambda_i f_i(x)\\) grows with \\(\\lambda_i\\) \u2192 violation becomes expensive.</li> </ul> <p>Geometric picture. At optimum, active constraints \u201cpush back\u201d against the objective\u2019s descent direction; multipliers are the weights of these pushes.</p>"},{"location":"duality_concept/#2-indicator-function-reformulation-extended-value-view","title":"2) Indicator-function reformulation (extended-value view)","text":"<p>Define indicators:</p> \\[ I_-(u)= \\begin{cases} 0, &amp; u\\le 0,\\\\ +\\infty, &amp; u&gt;0, \\end{cases} \\qquad I_0(u)= \\begin{cases} 0, &amp; u=0,\\\\ +\\infty, &amp; u\\ne 0. \\end{cases} \\] <p>Then the constrained problem is equivalent to:</p> \\[ \\min_x\\ \\Big( f_0(x)+\\sum_{i=1}^m I_-(f_i(x))+\\sum_{j=1}^p I_0(h_j(x)) \\Big). \\] <p>Convexity note. - If \\(f_0,f_i\\) are convex and \\(h_j\\) affine (standard convex form), this extended-value objective is convex. - If the original feasibility set is nonconvex (e.g., nonlinear equalities or nonconvex \\(f_i\\)), this form is generally nonconvex too (it encodes the same set).</p>"},{"location":"duality_concept/#3-why-replace-indicators-by-linear-underestimators","title":"3) Why replace indicators by linear underestimators?","text":"<p>For any \\(\\lambda\\ge 0\\) and any \\(u\\in\\mathbb{R}\\),</p> \\[ \\lambda u \\le I_-(u). \\] <p>For any \\(\\nu\\in\\mathbb{R}\\) and any \\(u\\in\\mathbb{R}\\),</p> \\[ \\nu u \\le I_0(u). \\] <p>So replacing indicator terms with \\(\\lambda_i f_i(x)\\) and \\(\\nu_j h_j(x)\\) produces a global lower bound on the extended-value objective.</p> <p>Key identity (supporting-hyperplane envelope).</p> \\[ I_-(u)=\\sup_{\\lambda\\ge 0}\\lambda u, \\qquad I_0(u)=\\sup_{\\nu\\in\\mathbb{R}}\\nu u. \\] <p>This is the deeper reason the \u201clinear estimator\u201d is canonical: the indicator is literally the pointwise supremum of these linear functionals.</p>"},{"location":"duality_concept/#4-dual-function-best-bound-from-lagrangian-relaxation","title":"4) Dual function = best bound from Lagrangian relaxation","text":"<p>Dual function</p> \\[ g(\\lambda,\\nu)=\\inf_x L(x,\\lambda,\\nu). \\] <p>Lower bound property (weak duality). For any feasible \\(x\\) and any \\(\\lambda\\ge 0\\),</p> \\[ g(\\lambda,\\nu)\\le L(x,\\lambda,\\nu)\\le f_0(x) \\quad \\Rightarrow \\quad g(\\lambda,\\nu)\\le p^\\star. \\] <p>Dual problem</p> \\[ \\max_{\\lambda\\ge 0,\\ \\nu\\in\\mathbb{R}^p}\\ g(\\lambda,\\nu). \\] <p>Interpretation: choose multipliers to make the lower bound as large (tight) as possible.</p>"},{"location":"duality_concept/#5-concavity-of-the-dual-function","title":"5) Concavity of the dual function","text":"<p>For each fixed \\(x\\), the function \\((\\lambda,\\nu)\\mapsto L(x,\\lambda,\\nu)\\) is affine. Therefore \\(g(\\lambda,\\nu)\\) is the pointwise infimum of affine functions, hence concave:</p> \\[ g(\\theta z_1+(1-\\theta)z_2)\\ge \\theta g(z_1)+(1-\\theta)g(z_2), \\qquad \\theta\\in[0,1]. \\] <p>Important distinction. - Pointwise supremum of convex functions is convex. - Pointwise infimum of affine functions is concave. - Pointwise infimum of general convex functions is not guaranteed convex/concave.</p>"},{"location":"duality_concept/#6-dual-is-convex-even-if-primal-is-nonconvex-what-is-true","title":"6) \u201cDual is convex even if primal is nonconvex\u201d \u2014 what is true?","text":"<p>The dual feasible set is typically just</p> \\[ \\lambda\\ge 0,\\qquad \\nu\\ \\text{free}. \\] <p>That set is convex regardless of primal convexity, and \\(g\\) is concave regardless of primal convexity, so the dual is a convex optimization problem in the dual variables.</p> <p>Two caveats. - Even though the dual is convex, evaluating \\(g(\\lambda,\\nu)=\\inf_x L(x,\\lambda,\\nu)\\) can be computationally hard. - Nonconvex primals can have a duality gap: \\(d^\\star &lt; p^\\star\\).</p>"},{"location":"duality_concept/#7-kkt-conditions-convex-case-regularity","title":"7) KKT conditions (convex case + regularity)","text":"<p>Assume convexity (convex \\(f_0,f_i\\), affine \\(h_j\\)) and a constraint qualification (e.g., Slater). Then \\(x^\\star,\\lambda^\\star,\\nu^\\star\\) are optimal iff:</p> <p>Primal feasibility</p> \\[ f_i(x^\\star)\\le 0,\\qquad h_j(x^\\star)=0. \\] <p>Dual feasibility</p> \\[ \\lambda^\\star\\ge 0. \\] <p>Stationarity</p> \\[ \\nabla f_0(x^\\star)+\\sum_{i=1}^m \\lambda_i^\\star \\nabla f_i(x^\\star)+\\sum_{j=1}^p \\nu_j^\\star \\nabla h_j(x^\\star)=0. \\] <p>Complementary slackness</p> \\[ \\lambda_i^\\star f_i(x^\\star)=0\\quad \\forall i. \\] <p>Geometric meaning: the objective gradient is balanced by a nonnegative combination of active constraint normals; inactive constraints get zero multiplier.</p>"},{"location":"duality_concept/#8-when-computing-glambdanu-is-np-hard","title":"8) When computing \\(g(\\lambda,\\nu)\\) is NP-hard","text":"<p>Computing \\(g\\) requires solving:</p> \\[ g(\\lambda,\\nu)=\\inf_x \\Big(f_0(x)+\\sum_i \\lambda_i f_i(x)+\\sum_j \\nu_j h_j(x)\\Big). \\] <p>This becomes NP-hard when the inner minimization is a hard global optimization problem, e.g.:</p> <ul> <li>Discrete/integer variables (MILP/MIQP structure).</li> <li>Nonconvex continuous minimization (indefinite QP over a polytope/box, general polynomial optimization).</li> <li>Nonlinear equalities producing nonconvex feasible geometry.</li> <li>Nonconvex \\(f_0\\) (already makes \\(g(0,0)=\\inf_x f_0(x)\\) hard in general).</li> </ul>"},{"location":"duality_concept/#9-when-duality-is-most-useful-practical-rule","title":"9) When duality is most useful (practical rule)","text":"<p>Duality is especially effective when: - \\(g(\\lambda,\\nu)\\) is efficiently computable (inner problem is convex or decomposes nicely), and - the duality gap is small (often zero in convex problems under Slater; sometimes small/zero due to special structure in nonconvex problems).</p> <p>Even with a nonconvex primal, a tractable dual can still provide strong lower bounds useful for: - certificates and benchmarking, - branch-and-bound pruning, - relaxation-based algorithms.</p>"},{"location":"duality_concept/#10-numerical-examples-primal-dual-variables","title":"10) Numerical examples (primal + dual variables)","text":""},{"location":"duality_concept/#example-a-1d-quadratic-with-box-constraints","title":"Example A: 1D quadratic with box constraints","text":"<p>Primal:</p> \\[ \\min_x\\ 3x^2-2x-1 \\quad \\text{s.t.}\\quad 0\\le x\\le 4. \\] <p>Unconstrained minimizer:</p> \\[ \\frac{d}{dx}(3x^2-2x-1)=6x-2=0 \\Rightarrow x^\\star=\\frac{1}{3}. \\] <p>Since \\(x^\\star\\in(0,4)\\), both constraints are inactive, so multipliers are zero:</p> \\[ \\lambda_{x\\le 4}^\\star=0,\\qquad \\lambda_{x\\ge 0}^\\star=0. \\] <p>Optimal value:</p> \\[ f(x^\\star)=3\\cdot\\frac{1}{9}-2\\cdot\\frac{1}{3}-1=-\\frac{4}{3}. \\]"},{"location":"duality_concept/#example-b-2d-convex-quadratic-with-multiple-active-constraints","title":"Example B: 2D convex quadratic with multiple active constraints","text":"<p>Primal:</p> \\[ \\min_{x,y}\\ (x-5)^2+(y-3)^2 \\] <p>s.t.</p> \\[ x+y\\le 3,\\qquad x\\le 1.5,\\qquad x\\ge 0,\\ y\\ge 0. \\] <p>Candidate optimum on active constraints \\(x=1.5\\) and \\(x+y=3\\):</p> \\[ x^\\star=1.5,\\qquad y^\\star=1.5. \\] <p>KKT multipliers: - active: \\(x+y\\le 3\\) with \\(\\lambda_1^\\star&gt;0\\), \\(x\\le 1.5\\) with \\(\\lambda_2^\\star&gt;0\\) - inactive: \\(x\\ge 0\\) and \\(y\\ge 0\\) so their multipliers are zero</p> <p>Stationarity at \\((1.5,1.5)\\):</p> \\[ \\nabla f(x,y)= \\begin{bmatrix} 2(x-5)\\\\ 2(y-3) \\end{bmatrix} \\Rightarrow \\nabla f(1.5,1.5)= \\begin{bmatrix} -7\\\\ -3 \\end{bmatrix}. \\] <p>Constraint normals: - for \\(x+y\\le 3\\): \\((1,1)\\) - for \\(x\\le 1.5\\): \\((1,0)\\)</p> <p>So</p> \\[ \\begin{bmatrix} -7\\\\ -3 \\end{bmatrix} +\\lambda_1 \\begin{bmatrix} 1\\\\ 1 \\end{bmatrix} +\\lambda_2 \\begin{bmatrix} 1\\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 0\\\\ 0 \\end{bmatrix}. \\] <p>From the \\(y\\)-component: \\(-3+\\lambda_1=0\\Rightarrow \\lambda_1^\\star=3\\).</p> <p>From the \\(x\\)-component: \\(-7+\\lambda_1+\\lambda_2=0\\Rightarrow \\lambda_2^\\star=4\\).</p> <p>Thus</p> \\[ (x^\\star,y^\\star)=(1.5,1.5), \\qquad \\lambda_1^\\star=3,\\quad \\lambda_2^\\star=4, \\] <p>and nonnegativity multipliers are \\(0\\).</p> <p>Objective value:</p> \\[ f(x^\\star,y^\\star)=(-3.5)^2+(-1.5)^2=12.25+2.25=14.5. \\]"},{"location":"duality_concept/#11-quick-mental-translation-rules","title":"11) Quick mental \u201ctranslation rules\u201d","text":"<ul> <li>Lagrangian = objective + priced constraints:</li> </ul> <p>$$   L=f_0+\\lambda^\\top f+\\nu^\\top h.   $$</p> <ul> <li>Dual function is a bound:</li> </ul> <p>$$   g(\\lambda,\\nu)=\\inf_x L(x,\\lambda,\\nu)\\le p^\\star.   $$</p> <ul> <li>Concavity comes from \u201cinf of affine\u201d:</li> </ul> <p>$$   g \\ \\text{concave in }(\\lambda,\\nu).   $$</p> <ul> <li>Dual feasible set is convex even if primal is not:</li> </ul> <p>$$   \\lambda\\ge 0,\\ \\nu\\ \\text{free}.   $$</p> <ul> <li>Duality is most computationally useful when the inner infimum is tractable and the gap is small.</li> </ul>"}]}